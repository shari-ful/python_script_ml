{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### install mnist library\n",
        "This Library used to load mnist dataset"
      ],
      "metadata": {
        "id": "NICQNix5t6NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aglYIRdfpTdY",
        "outputId": "658a48cd-2aa1-4e41-9844-d1b293c7cbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mnist\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mnist) (1.23.5)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import mnist"
      ],
      "metadata": {
        "id": "MXLmDTRmuLx-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upRqFcBwGF2O"
      },
      "outputs": [],
      "source": [
        "import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training dataset and test dataset from nist dataset\n",
        "\n",
        "\n",
        "*   x_train: training images\n",
        "*   x_test: test images\n",
        "*   y_train: training label\n",
        "*   y_test: test label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nLPTO8VXuVot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = mnist.train_images(), mnist.test_images(), mnist.train_labels(), mnist.test_labels()"
      ],
      "metadata": {
        "id": "rSIF8nLxnfDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVYz4l1Fnicb",
        "outputId": "3be7d224-4523-42a9-af04-fddd69f99063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT_MHO12p7hA",
        "outputId": "e00f5dae-b855-4dbd-d2af-e15b96d3442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pixel normalization for training and test dataset"
      ],
      "metadata": {
        "id": "N1xH8R5Xvc3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255, x_test / 255"
      ],
      "metadata": {
        "id": "CAICzszMnxt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H2-eRa_6vldO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "jAdxe4DZrQbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorize training and test labels"
      ],
      "metadata": {
        "id": "c13ODbSmvxJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
      ],
      "metadata": {
        "id": "U76Uj1q_q6H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "be_dmYImrw5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### neural network model using the TensorFlow and Keras libraries.\n",
        "\n",
        "*  **models.Sequential**: This creates a sequential model, which is a linear stack of layers where we can add one layer at a time. In this context, it's used to define the architecture of the neural network.\n",
        "\n",
        "*  **layers.Flatten(input_shape=(28, 28))**: This is the input layer of the neural network. It takes an input with a shape of (28, 28), which corresponds to 28x28 pixel grayscale images. The **Flatten** layer is responsible for converting the 2D array of pixel values into a 1D array by flattening it. This is necessary because the subsequent layers in the neural network expect a 1D input.\n",
        "\n",
        "*  **layers.Dense(128, activation='relu')**: This is the first hidden layer of the neural network. It is a fully connected (Dense) layer with 128 neurons. The 'relu' activation function (Rectified Linear Unit) is applied to the output of this layer. The purpose of this layer is to learn complex patterns in the data by applying a linear transformation followed by the activation function.\n",
        "\n",
        "*  **layers.Dropout(0.2)**: This is a dropout layer. Dropout is a regularization technique that helps prevent overfitting. It randomly sets a fraction (in this case, 20%) of the input units to 0 during training, which helps to reduce the co-dependency of neurons and makes the network more robust.\n",
        "\n",
        "*  **layers.Dense(10, activation='softmax')**: This is the output layer of the neural network. It is another fully connected layer with 10 neurons, one for each possible digit (0 to 9). The 'softmax' activation function is applied to this layer, which converts the raw output values into a probability distribution over the 10 classes. This means the output values will represent the probabilities of the input image belonging to each digit class.\n",
        "\n",
        "this neural network model takes 28x28 pixel images as input, flattens them into a 1D array, passes them through a hidden layer with 128 neurons and ReLU activation, applies dropout for regularization, and finally, produces output probabilities for each of the 10 digit classes using the softmax activation function. This architecture is suitable for the MNIST dataset, which is a collection of handwritten digit images that need to be classified into one of the 10 digits."
      ],
      "metadata": {
        "id": "YUmJ3XGXwA5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "PPk3kC-7rrbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the training process for a neural network model.\n",
        "\n",
        "*  **optimizer='adam'**:\n",
        "   - The **optimizer** argument specifies the optimization algorithm used to update the model's weights during training. In this case, it is set to 'adam', which stands for Adaptive Moment Estimation. Adam is a popular optimization algorithm known for its efficiency and ability to adapt learning rates during training.\n",
        "\n",
        "*  **loss='categorical_crossentropy'**:\n",
        "   - The **loss** argument defines the loss function that the model will use to measure how well it's performing during training. For classification problems with multiple classes, 'categorical_crossentropy' is a common choice. It measures the dissimilarity between the true class probabilities (one-hot encoded labels) and the predicted probabilities generated by the model. The goal during training is to minimize this loss, which essentially means making the predicted probabilities as close as possible to the true class probabilities.\n",
        "\n",
        "3. `metrics=['accuracy']`:\n",
        "   - The `metrics` argument is a list of metrics used to evaluate the model's performance during and after training. In this case, 'accuracy' is specified, which is a common metric for classification tasks. Accuracy measures the fraction of correctly classified samples over the total number of samples. During training, the model will compute and display this metric so you can monitor how well it's learning to classify the data.\n",
        "\n",
        "**model.compile** is a critical step in setting up the neural network for training. It configures the optimization algorithm, specifies the loss function to optimize, and defines evaluation metrics to track the model's performance. Once the model is compiled, we can start training it on the dataset using the **fit** method, and it will use the specified optimizer and loss function to learn from the data while providing accuracy as a performance metric to assess its training progress."
      ],
      "metadata": {
        "id": "zeMDHOoCxVr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dnoxQL9TqHU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train neural network model on a given dataset.\n",
        "\n",
        "*  **x_train and y_train**:\n",
        "   - **x_train** represents the input data (features) that we want to use to train the model. This typically consists of a set of input samples, such as images in the case of image classification.\n",
        "   - **y_train** represents the corresponding target labels or ground truth for the input data. These are the correct answers or labels associated with each input sample. In classification tasks, **y_train** usually consists of one-hot encoded labels or integer values representing the class of each input sample.\n",
        "\n",
        "*  **epochs=5**:\n",
        "   - The **epochs** argument specifies the number of times the entire training dataset (i.e., **x_train and y_train**) should be passed forward and backward through the neural network. Each pass through the entire dataset is called an \"epoch.\" In this case, the training process will go through the dataset 5 times.\n",
        "\n",
        "*  **batch_size=64**:\n",
        "   - The **batch_size** argument determines how many samples are used in each update of the model's weights. Instead of updating the weights after each individual sample (which can be computationally inefficient), training is typically done in batches. Here, the batch size is set to 64, meaning that the model's weights will be updated once every 64 samples.\n",
        "\n",
        "*  **validation_split=0.2**:\n",
        "   - The **validation_split** argument is used to specify the fraction of the training data that should be reserved for validation. In this case, 20% of the training data (**x_train and y_train**) will be set aside for validation purposes. This validation data is not used for training but is used to monitor the model's performance during training, helping to detect overfitting.\n",
        "\n",
        "When we execute **model.fit**, the training process begins. During each epoch, the following steps occur:\n",
        "\n",
        "- The training data (**x_train and y_train**) is divided into batches of size specified by **batch_size**.\n",
        "- For each batch, the model computes predictions on the input data.\n",
        "- The loss function specified during model compilation (in this case, 'categorical_crossentropy') is used to compute the difference between the predicted values and the true labels (i.e., the training loss).\n",
        "- The optimization algorithm (Adam in this case) adjusts the model's weights to minimize this loss.\n",
        "- After processing all batches in an epoch, the model's performance on the validation data (if **validation_split** is provided) is evaluated using the same loss and any specified metrics (e.g., 'accuracy').\n",
        "- Training progresses through the specified number of epochs (5 in this case).\n",
        "\n",
        "At the end of training, we will have a trained neural network model that has hopefully learned to make accurate predictions on our data. we can then use this trained model for making predictions on new, unseen data."
      ],
      "metadata": {
        "id": "BoZicoPOyetB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPETzo6jqI7o",
        "outputId": "99a118f1-bdc2-4dd5-fdc2-debcfee64ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.3742 - accuracy: 0.8937 - val_loss: 0.1721 - val_accuracy: 0.9521\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1749 - accuracy: 0.9499 - val_loss: 0.1307 - val_accuracy: 0.9604\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1310 - accuracy: 0.9616 - val_loss: 0.1084 - val_accuracy: 0.9675\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1071 - accuracy: 0.9678 - val_loss: 0.0957 - val_accuracy: 0.9692\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.0885 - accuracy: 0.9729 - val_loss: 0.0860 - val_accuracy: 0.9741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79191fbe50c0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate a trained neural network model on a separate test dataset to assess its performance.\n",
        "\n",
        "*  **x_test and y_test**:\n",
        "   - **x_test** represents the test data, which is a set of input samples that the model has never seen during training or validation. It is used to evaluate how well the model generalizes to new, unseen data.\n",
        "   - **y_test** represents the corresponding target labels or ground truth for the test data. These are the correct answers or labels associated with each input sample in **x_test**.\n",
        "\n",
        "*  **model.evaluate(x_test, y_test)**:\n",
        "   - The **model.evaluate** method takes the test data **x_test and y_test** as inputs and computes two main quantities:\n",
        "     - **test_loss**: This is the value of the loss function (specified during model compilation, typically 'categorical_crossentropy' for classification tasks) on the test dataset. It quantifies how well the model's predictions match the true labels on the test data. Lower values indicate better performance.\n",
        "     - **test_accuracy**: This is the accuracy of the model on the test dataset. It measures the fraction of correctly classified samples in the test data.\n",
        "\n",
        "*  **Assignment**:\n",
        "   - The results of **model.evaluate** are assigned to the variables **test_loss and test_accuracy** for further analysis or reporting.\n",
        "\n",
        "This allows us to assess how well our trained neural network model performs on unseen data (the test dataset). It calculates the loss and accuracy of the model on this test data, providing a quantitative measure of its generalization and predictive performance. These metrics are crucial for evaluating the model's suitability for real-world applications and for comparing different models or configurations."
      ],
      "metadata": {
        "id": "N-_jrDP00UhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBB8qCs5sXOA",
        "outputId": "a38928ae-2d1e-423c-e4e8-39155737e088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### print the accuracy"
      ],
      "metadata": {
        "id": "1GwbfSmU1aLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FrWAu3ssiUO",
        "outputId": "bae1bdfc-53dd-4f11-dfed-a022a19be239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9742000102996826"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model as a pickle file\n",
        "with open('mnist_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "MtrAixE_GorK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}